hold_set = hold_out_scaled,
task = task,
demo_set = own_demo)
#return(features)
}
setwd('~/Dánsko/uni/4th semester/Social and Cultural Dynamics/exam/r_code')
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071"))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
#library(glmnet)
library(data.table)
library(broom)
library(forcats)
library(e1071)
source("functions/new_partitioning.R")
source("functions/Normalize_function.R")
source("functions/feature_selection.R")
source("functions/combine_dfs.R")
source("functions/id_wrangl.R")
grand_function <- function(features,
other_dataframe,
demo,
lang,
task,
featureset,
n_lasso_folds = 5){
set.seed(1234)
###Cleaning the data###
demo <- demo %>% filter(Gender == "Male")
id_clean <- id_wrangling(features, demo, other_dataframe, language = lang)
features <- id_clean[[1]]
other_dataframe <- id_clean[[2]]
demo <- id_clean[[3]]
#Combining with demo
own_demo = filter(demo, language == lang)
other_demo = filter(demo, language != lang)
features <- combined_data(data = features, demo = own_demo)
other_dataframe <- combined_data(data = other_dataframe, demo = other_demo)
###relevant task
features_task <- features %>% filter(condition == task)
features_other_task <- features %>% filter(condition != task)
###partitioning###
if (lang == "dk"){
partitions <- partition_dk(features = features_task,
demo = own_demo)
}
if (lang == "us"){
partitions <- partition_us(features = features_task,
demo = own_demo)
}
train <- partitions[[2]]
hold_out <- partitions[[1]]
#normalizing - all datasets are normalized according to the train set
train_scaled <- as.data.frame(
scale_function(train, datatype = "train"))
hold_out_scaled <- as.data.frame(
scale_function(train,
hold_out,
datatype = "test"))
hold_out_other_scaled <- as.data.frame(
scale_function(train,
other_dataframe,
datatype = "test"))
features_other_task_scaled <- as.data.frame(
scale_function(train,
features_other_task,
datatype = "test"))
# ###Saving dataframe with other language after scaling
if (lang == "dk"){
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
if (lang == "us"){
hold_out_other_scaled <- hold_out_other_scaled %>% filter(condition == task)
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/", featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
# #Saving dataframe with other task after scaling
if (lang == "dk"){
write.csv(features_other_task_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on" , "not", task, sep = "_"),
"csv", sep = "."))
}
# #Saving hold out set from same task
write.csv(hold_out_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", lang, task, sep = "_"),
"csv", sep = "."))
train_scaled$ID <- as.factor(train_scaled$ID)
# # ##Elastic net ###
elastic(train_data = train_scaled,
folds = n_lasso_folds,
id_col = "ID",
featureset = featureset,
language = lang,
hold_set = hold_out_scaled,
task = task,
demo_set = own_demo)
#return(features)
}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Dánsko/uni/4th semester/Social and Cultural Dynamics/exam/r_code")
require(devtools)
#install_version(devtools, version = "1.0.7")
packageurl <- "https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_1.0.0.tar.gz"
install.packages(packageurl, repos=NULL, type="source")
packageVersion("dplyr")
source("grand_function.R")
egemaps_dk <- read.csv("../data/R_data/egemaps_dk.csv", sep=",")
egemaps_us <- read.csv("../data/R_data/egemaps_us.csv", sep=",")
demodata   <- read.csv("../data/R_data/DemoData.csv", sep=",")
# remove index (X) collumn
egemaps_dk <- egemaps_dk[,2:ncol(egemaps_dk)]
egemaps_us <- egemaps_us[,2:ncol(egemaps_us)]
source("grand_function.R")
grand_function(features = egemaps_dk,
other_dataframe = egemaps_us,
demo = demodata,
lang = "dk",
task = "stories",
featureset = "egemaps",
n_lasso_folds = 5)
grand_function(features = egemaps_dk,
other_dataframe = egemaps_us,
demo = demodata,
lang = "dk",
task = "triangles",
featureset = "egemaps",
n_lasso_folds = 5)
grand_function(features = egemaps_us,
other_dataframe = egemaps_dk,
demo = demodata,
lang = "us",
task = "stories",
featureset = "egemaps",
n_lasso_folds = 5)
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_1.0.0.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#packageVersion("dplyr")
source("grand_function.R")
egemaps_dk <- read.csv("../data/R_data/egemaps_dk.csv", sep=",")
egemaps_us <- read.csv("../data/R_data/egemaps_us.csv", sep=",")
demodata   <- read.csv("../data/R_data/DemoData.csv", sep=",")
# remove index (X) collumn
egemaps_dk <- egemaps_dk[,2:ncol(egemaps_dk)]
egemaps_us <- egemaps_us[,2:ncol(egemaps_us)]
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Dánsko/uni/4th semester/Social and Cultural Dynamics/exam")
require(devtools)
#install_version(devtools, version = "1.0.7")
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_1.0.0.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#packageVersion("dplyr")
source("grand_function.R")
egemaps_dk <- read.csv("../data/R_data/egemaps_dk.csv", sep=",")
egemaps_us <- read.csv("../data/R_data/egemaps_us.csv", sep=",")
demodata   <- read.csv("../data/R_data/DemoData.csv", sep=",")
# remove index (X) collumn
egemaps_dk <- egemaps_dk[,2:ncol(egemaps_dk)]
egemaps_us <- egemaps_us[,2:ncol(egemaps_us)]
source("grand_function.R")
grand_function(features = egemaps_dk,
other_dataframe = egemaps_us,
demo = demodata,
lang = "dk",
task = "stories",
featureset = "egemaps",
n_lasso_folds = 5)
getwd()
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071"))
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071"))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
#library(glmnet)
library(data.table)
library(broom)
library(forcats)
library(e1071)
source("functions/new_partitioning.R")
source("functions/Normalize_function.R")
source("functions/feature_selection.R")
source("functions/combine_dfs.R")
source("functions/id_wrangl.R")
grand_function <- function(features,
other_dataframe,
demo,
lang,
task,
featureset,
n_lasso_folds = 5){
set.seed(1234)
###Cleaning the data###
demo <- demo %>% filter(Gender == "Male")
id_clean <- id_wrangling(features, demo, other_dataframe, language = lang)
features <- id_clean[[1]]
other_dataframe <- id_clean[[2]]
demo <- id_clean[[3]]
#Combining with demo
own_demo = filter(demo, language == lang)
other_demo = filter(demo, language != lang)
features <- combined_data(data = features, demo = own_demo)
other_dataframe <- combined_data(data = other_dataframe, demo = other_demo)
###relevant task
features_task <- features %>% filter(condition == task)
features_other_task <- features %>% filter(condition != task)
###partitioning###
if (lang == "dk"){
partitions <- partition_dk(features = features_task,
demo = own_demo)
}
if (lang == "us"){
partitions <- partition_us(features = features_task,
demo = own_demo)
}
train <- partitions[[2]]
hold_out <- partitions[[1]]
#normalizing - all datasets are normalized according to the train set
train_scaled <- as.data.frame(
scale_function(train, datatype = "train"))
hold_out_scaled <- as.data.frame(
scale_function(train,
hold_out,
datatype = "test"))
hold_out_other_scaled <- as.data.frame(
scale_function(train,
other_dataframe,
datatype = "test"))
features_other_task_scaled <- as.data.frame(
scale_function(train,
features_other_task,
datatype = "test"))
# ###Saving dataframe with other language after scaling
if (lang == "dk"){
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
if (lang == "us"){
hold_out_other_scaled <- hold_out_other_scaled %>% filter(condition == task)
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/", featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
# #Saving dataframe with other task after scaling
if (lang == "dk"){
write.csv(features_other_task_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on" , "not", task, sep = "_"),
"csv", sep = "."))
}
# #Saving hold out set from same task
write.csv(hold_out_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", lang, task, sep = "_"),
"csv", sep = "."))
train_scaled$ID <- as.factor(train_scaled$ID)
# # ##Elastic net ###
elastic(train_data = train_scaled,
folds = n_lasso_folds,
id_col = "ID",
featureset = featureset,
language = lang,
hold_set = hold_out_scaled,
task = task,
demo_set = own_demo)
#return(features)
}
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071"))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
#library(glmnet)
library(data.table)
library(broom)
library(forcats)
library(e1071)
source("functions/new_partitioning.R")
source("functions/Normalize_function.R")
source("functions/feature_selection.R")
source("functions/combine_dfs.R")
source("functions/id_wrangl.R")
grand_function <- function(features,
other_dataframe,
demo,
lang,
task,
featureset,
n_lasso_folds = 5){
set.seed(1234)
###Cleaning the data###
demo <- demo %>% filter(Gender == "Male")
id_clean <- id_wrangling(features, demo, other_dataframe, language = lang)
features <- id_clean[[1]]
other_dataframe <- id_clean[[2]]
demo <- id_clean[[3]]
#Combining with demo
own_demo = filter(demo, language == lang)
other_demo = filter(demo, language != lang)
features <- combined_data(data = features, demo = own_demo)
other_dataframe <- combined_data(data = other_dataframe, demo = other_demo)
###relevant task
features_task <- features %>% filter(condition == task)
features_other_task <- features %>% filter(condition != task)
###partitioning###
if (lang == "dk"){
partitions <- partition_dk(features = features_task,
demo = own_demo)
}
if (lang == "us"){
partitions <- partition_us(features = features_task,
demo = own_demo)
}
train <- partitions[[2]]
hold_out <- partitions[[1]]
#normalizing - all datasets are normalized according to the train set
train_scaled <- as.data.frame(
scale_function(train, datatype = "train"))
hold_out_scaled <- as.data.frame(
scale_function(train,
hold_out,
datatype = "test"))
hold_out_other_scaled <- as.data.frame(
scale_function(train,
other_dataframe,
datatype = "test"))
features_other_task_scaled <- as.data.frame(
scale_function(train,
features_other_task,
datatype = "test"))
# ###Saving dataframe with other language after scaling
if (lang == "dk"){
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
if (lang == "us"){
hold_out_other_scaled <- hold_out_other_scaled %>% filter(condition == task)
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/", featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
# #Saving dataframe with other task after scaling
if (lang == "dk"){
write.csv(features_other_task_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on" , "not", task, sep = "_"),
"csv", sep = "."))
}
# #Saving hold out set from same task
write.csv(hold_out_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", lang, task, sep = "_"),
"csv", sep = "."))
train_scaled$ID <- as.factor(train_scaled$ID)
# # ##Elastic net ###
elastic(train_data = train_scaled,
folds = n_lasso_folds,
id_col = "ID",
featureset = featureset,
language = lang,
hold_set = hold_out_scaled,
task = task,
demo_set = own_demo)
#return(features)
}
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071", 'glmnet'))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
#library(glmnet)
library(data.table)
library(broom)
library(forcats)
library(e1071)
source("functions/new_partitioning.R")
source("functions/Normalize_function.R")
source("functions/feature_selection.R")
source("functions/combine_dfs.R")
source("functions/id_wrangl.R")
grand_function <- function(features,
other_dataframe,
demo,
lang,
task,
featureset,
n_lasso_folds = 5){
set.seed(1234)
###Cleaning the data###
demo <- demo %>% filter(Gender == "Male")
id_clean <- id_wrangling(features, demo, other_dataframe, language = lang)
features <- id_clean[[1]]
other_dataframe <- id_clean[[2]]
demo <- id_clean[[3]]
#Combining with demo
own_demo = filter(demo, language == lang)
other_demo = filter(demo, language != lang)
features <- combined_data(data = features, demo = own_demo)
other_dataframe <- combined_data(data = other_dataframe, demo = other_demo)
###relevant task
features_task <- features %>% filter(condition == task)
features_other_task <- features %>% filter(condition != task)
###partitioning###
if (lang == "dk"){
partitions <- partition_dk(features = features_task,
demo = own_demo)
}
if (lang == "us"){
partitions <- partition_us(features = features_task,
demo = own_demo)
}
train <- partitions[[2]]
hold_out <- partitions[[1]]
#normalizing - all datasets are normalized according to the train set
train_scaled <- as.data.frame(
scale_function(train, datatype = "train"))
hold_out_scaled <- as.data.frame(
scale_function(train,
hold_out,
datatype = "test"))
hold_out_other_scaled <- as.data.frame(
scale_function(train,
other_dataframe,
datatype = "test"))
features_other_task_scaled <- as.data.frame(
scale_function(train,
features_other_task,
datatype = "test"))
# ###Saving dataframe with other language after scaling
if (lang == "dk"){
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
if (lang == "us"){
hold_out_other_scaled <- hold_out_other_scaled %>% filter(condition == task)
write.csv(hold_out_other_scaled,
paste(paste("../data/speech_data/", featureset, "model", lang, task, "test_on", "not", lang, sep = "_"),
"csv", sep = "."))
}
# #Saving dataframe with other task after scaling
if (lang == "dk"){
write.csv(features_other_task_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on" , "not", task, sep = "_"),
"csv", sep = "."))
}
# #Saving hold out set from same task
write.csv(hold_out_scaled,
paste(paste("../data/speech_data/",featureset, "model", lang, task, "test_on", lang, task, sep = "_"),
"csv", sep = "."))
train_scaled$ID <- as.factor(train_scaled$ID)
# # ##Elastic net ###
elastic(train_data = train_scaled,
folds = n_lasso_folds,
id_col = "ID",
featureset = featureset,
language = lang,
hold_set = hold_out_scaled,
task = task,
demo_set = own_demo)
#return(features)
}
#library(pacman)
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071", 'glmnet'))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
library(glmnet)
#library(glmnet)
library(data.table)
View(demodata)
View(egemaps_dk)
View(egemaps_us)
# glmnet is tricky - see below
install.packages('glmnet')
install.packages('glmnet')
# glmnet is tricky - see below
remotes::install_version('glmnet', version = '2.0-18')
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Dánsko/uni/4th semester/Social and Cultural Dynamics/exam")
install.packages("devtools", repos="https://cran.rstudio.com/")
#install_version(devtools, version = "1.0.7")
#packageurl <- "https://cran.r-project.org/src/contrib/Archive/dplyr/dplyr_1.0.0.tar.gz"
#install.packages(packageurl, repos=NULL, type="source")
#packageVersion("dplyr")
source("grand_function.R")
#pacman::p_load(tidyverse, readr, glmnet, data.table, broom, forcats, e1071, cvms)
install.packages(c("tidyverse", "readr", "data.table", "broom", "forcats", "e1071", 'glmnet'))
# glmnet is tricky - see below
library(tidyverse)
library(readr)
library(glmnet)
install.packages("glmnet")
help(glmnet)
??glmnet
# Load libraries
library(tidyverse)
library(readr)
library(glmnet)
version
chooseCRANmirror()
install.packages("glmnet")
install.packages("MASS")
